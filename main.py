import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime
import re
import base64
import time
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# æ£€æŸ¥å¿…éœ€çš„åº“
def check_dependencies():
    """æ£€æŸ¥å¹¶æç¤ºå®‰è£…å¿…éœ€çš„ä¾èµ–"""
    missing = []
    try:
        import brotli
    except ImportError:
        try:
            import brotlicffi
        except ImportError:
            missing.append("brotli")
    
    if missing:
        print("âš ï¸  æ£€æµ‹åˆ°ç¼ºå°‘å¿…éœ€çš„ä¾èµ–åº“:")
        for lib in missing:
            print(f"   - {lib}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print("   pip install brotli")
        print("\næˆ–è€…å®‰è£…æ‰€æœ‰ä¾èµ–:")
        print("   pip install -r requirements.txt")
        print("")
        return False
    return True

# åŸŸåé…ç½®
LOCAL_DOMAIN = ""                     # æœ¬åœ°æµ‹è¯•æ—¶å¯å¡«å…¥åŸŸåï¼Œå¦‚ï¼šikuuu.org
DEFAULT_DOMAIN = "ikuuu.ch"           # é»˜è®¤åŸŸå

# åˆå§‹å€¼ï¼Œä¼šè¢« resolve_domain() è¦†ç›–
BASE_DOMAIN = DEFAULT_DOMAIN
BASE_URL = f"https://{BASE_DOMAIN}"

# åŸŸåè‡ªåŠ¨å‘ç°é…ç½®
DOMAIN_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "domain.txt")
NAVIGATION_URLS = [
    "https://ikuuu.ch",
]
DOMAIN_TEST_TIMEOUT = 5

# æœ¬åœ°æµ‹è¯•å˜é‡ï¼Œæœ¬åœ°æµ‹è¯•æ—¶å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®ï¼Œç¯å¢ƒå˜é‡ä¼˜å…ˆçº§æ›´é«˜
LOCAL_EMAIL = ""     # æœ¬åœ°æµ‹è¯•æ—¶å¡«å…¥é‚®ç®±
LOCAL_PASSWORD = ""  # æœ¬åœ°æµ‹è¯•æ—¶å¡«å…¥å¯†ç 

def print_with_time(message, level="INFO"):
    """å¸¦æ—¶é—´æˆ³å’Œçº§åˆ«çš„æ‰“å°"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    level_emoji = {
        "INFO": "â„¹ï¸",
        "SUCCESS": "âœ…", 
        "WARNING": "âš ï¸",
        "ERROR": "âŒ",
        "DEBUG": "ğŸ”"
    }
    emoji = level_emoji.get(level, "â„¹ï¸")
    print(f"[{current_time}] {emoji} {message}")

def print_separator(char="=", length=60):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)

def read_domain_from_file():
    """ä» domain.txt è¯»å–ç¼“å­˜çš„åŸŸå"""
    try:
        if os.path.exists(DOMAIN_FILE):
            with open(DOMAIN_FILE, 'r', encoding='utf-8') as f:
                domain = f.readline().strip()
            if domain and '.' in domain and ' ' not in domain:
                domain = domain.replace('https://', '').replace('http://', '').rstrip('/')
                print_with_time(f"ä»ç¼“å­˜æ–‡ä»¶è¯»å–åŸŸå: {domain}", "DEBUG")
                return domain
            else:
                print_with_time(f"ç¼“å­˜æ–‡ä»¶ä¸­çš„åŸŸåæ— æ•ˆ: '{domain}'", "WARNING")
    except Exception as e:
        print_with_time(f"è¯»å–åŸŸåç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}", "WARNING")
    return None

def save_domain_to_file(domain):
    """å°†å¯ç”¨åŸŸåä¿å­˜åˆ° domain.txt"""
    try:
        with open(DOMAIN_FILE, 'w', encoding='utf-8') as f:
            f.write(domain.strip() + '\n')
        print_with_time(f"å·²ä¿å­˜åŸŸååˆ°ç¼“å­˜æ–‡ä»¶: {domain}", "SUCCESS")
        return True
    except Exception as e:
        print_with_time(f"ä¿å­˜åŸŸåç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}", "WARNING")
        return False

def parse_json_response(response, context="å“åº”"):
    """å®‰å…¨åœ°è§£æJSONå“åº”"""
    import json
    try:
        return response.json()
    except Exception:
        # requests å·²è‡ªåŠ¨å¤„ç† gzip/brotli è§£å‹ï¼Œåªéœ€æ¸…ç†æ–‡æœ¬
        text = response.text.lstrip('\ufeff')
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            return json.loads(match.group())
        raise ValueError(f"{context}å“åº”ä¸å«æœ‰æ•ˆJSON")

def create_session():
    """åˆ›å»ºé…ç½®å®Œæ•´çš„ä¼šè¯å¯¹è±¡"""
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    
    # è®¾ç½®é€‚é…å™¨ï¼Œé¿å…è¿æ¥æ± é—®é¢˜
    adapter = requests.adapters.HTTPAdapter(pool_connections=1, pool_maxsize=1)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    return session

def test_domain(domain):
    """å¿«é€Ÿæµ‹è¯•åŸŸåæ˜¯å¦ä¸ºå¯ç”¨çš„ ikuuu æœåŠ¡ï¼ˆéå¯¼èˆªé¡µï¼‰

    åˆ¤æ–­é€»è¾‘ï¼šGET æ ¹è·¯å¾„ï¼ŒçœŸå®æœåŠ¡ä¼š 302 è·³è½¬åˆ° /auth/loginï¼Œå¯¼èˆªé¡µè¿”å› 200
    """
    test_url = f"https://{domain}/"
    try:
        session = create_session()
        response = session.get(test_url, timeout=DOMAIN_TEST_TIMEOUT, verify=False, allow_redirects=False)
        session.close()

        # çœŸå®æœåŠ¡ï¼šæ ¹è·¯å¾„ 302 è·³è½¬åˆ° /auth/login
        if response.status_code == 302:
            location = response.headers.get('Location', '')
            if '/auth/login' in location:
                print_with_time(f"åŸŸå {domain} å¯ç”¨ï¼ˆ302 -> /auth/loginï¼‰", "SUCCESS")
                return True
            else:
                print_with_time(f"åŸŸå {domain} 302è·³è½¬åˆ° {location}ï¼ŒéæœåŠ¡é¡µé¢", "DEBUG")
                return False

        # å¯¼èˆªé¡µï¼šè¿”å› 200 çš„ HTML é¡µé¢
        if response.status_code == 200:
            print_with_time(f"åŸŸå {domain} è¿”å›200ï¼Œå¯èƒ½æ˜¯å¯¼èˆªé¡µ", "DEBUG")
            return False

        print_with_time(f"åŸŸå {domain} è¿”å›çŠ¶æ€ç  {response.status_code}", "WARNING")
        return False
    except Exception as e:
        print_with_time(f"åŸŸå {domain} ä¸å¯ç”¨: {str(e)}", "DEBUG")
        return False

def _decode_obfuscated_strings(html_text):
    """ä»å¯¼èˆªé¡µçš„æ··æ·†JSä¸­è§£ç å­—ç¬¦ä¸²æ•°ç»„"""
    # è‡ªå®šä¹‰ base64 å­—æ¯è¡¨ï¼ˆå°å†™åœ¨å‰ï¼Œå¤§å†™åœ¨åï¼‰
    custom = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+/='
    standard = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/='
    table = str.maketrans(custom, standard)

    decoded = []
    # æå–ç¼–ç å­—ç¬¦ä¸²æ•°ç»„
    arrays = re.findall(r"\[(?:'[^']*',?\s*){5,}\]", html_text)
    for arr in arrays:
        items = re.findall(r"'([^']*)'", arr)
        for s in items:
            try:
                translated = s.translate(table)
                padding = 4 - len(translated) % 4
                if padding != 4:
                    translated += '=' * padding
                raw = base64.b64decode(translated)
                decoded.append(raw.decode('utf-8', errors='ignore'))
            except Exception:
                pass
    return decoded

def discover_domains():
    """ä»å¯¼èˆªé¡µè‡ªåŠ¨å‘ç°å½“å‰å¯ç”¨åŸŸååˆ—è¡¨"""
    print_with_time("å¼€å§‹è‡ªåŠ¨å‘ç°åŸŸå...", "INFO")
    discovered = []

    for nav_url in NAVIGATION_URLS:
        try:
            print_with_time(f"å°è¯•ä» {nav_url} è·å–åŸŸååˆ—è¡¨...", "DEBUG")
            session = create_session()
            response = session.get(nav_url, timeout=DOMAIN_TEST_TIMEOUT + 5, verify=False, allow_redirects=True)
            session.close()

            if response.status_code != 200:
                continue

            html_text = response.text

            # ç­–ç•¥1ï¼šä» h3 æ ‡ç­¾æå–åŸŸåï¼ˆé€‚ç”¨äºéJSæ¸²æŸ“çš„å¯¼èˆªé¡µï¼‰
            soup = BeautifulSoup(html_text, 'html.parser')
            for h3 in soup.find_all('h3'):
                text = h3.get_text(strip=True)
                if re.match(r'^ikuuu\.\w{2,}$', text, re.IGNORECASE):
                    discovered.append(text.lower())

            # ç­–ç•¥2ï¼šä» a æ ‡ç­¾ href æå–åŸŸå
            for a in soup.find_all('a', href=True):
                match = re.search(r'https?://(ikuuu\.\w{2,})/?', a['href'], re.IGNORECASE)
                if match:
                    domain = match.group(1).lower()
                    if domain not in discovered:
                        discovered.append(domain)

            # ç­–ç•¥3ï¼šä»å­—ç¬¦ä¸²æ‹¼æ¥æ¨¡å¼æå–ï¼ˆå¦‚ 'ikuuu'+'.nl'ï¼‰
            for m in re.finditer(r"'ikuuu'\+'\.(\w{2,})'", html_text):
                domain = f"ikuuu.{m.group(1).lower()}"
                if domain not in discovered:
                    discovered.append(domain)

            # ç­–ç•¥4ï¼šè§£ç æ··æ·†JSå­—ç¬¦ä¸²æ•°ç»„ï¼ŒæŸ¥æ‰¾ TLD ç‰‡æ®µï¼ˆå¦‚ .fyi, .nlï¼‰
            for s in _decode_obfuscated_strings(html_text):
                if re.match(r'^\.\w{2,4}$', s):
                    domain = f"ikuuu{s}".lower()
                    if domain not in discovered:
                        discovered.append(domain)

            if discovered:
                # è¿‡æ»¤æ‰å¯¼èˆªé¡µè‡ªèº«çš„åŸŸå
                nav_domain = nav_url.replace('https://', '').replace('http://', '').rstrip('/')
                discovered = [d for d in discovered if d != nav_domain]
                print_with_time(f"å‘ç° {len(discovered)} ä¸ªåŸŸå: {', '.join(discovered)}", "SUCCESS")
                break

        except Exception as e:
            print_with_time(f"ä» {nav_url} è·å–åŸŸåå¤±è´¥: {str(e)}", "DEBUG")
            continue

    if not discovered:
        print_with_time("è‡ªåŠ¨åŸŸåå‘ç°æœªæ‰¾åˆ°ä»»ä½•åŸŸå", "WARNING")

    return discovered

def _set_domain(domain):
    """è®¾ç½®å½“å‰ä½¿ç”¨çš„åŸŸå"""
    global BASE_DOMAIN, BASE_URL
    BASE_DOMAIN = domain
    BASE_URL = f"https://{BASE_DOMAIN}"

def resolve_domain():
    """æŒ‰ä¼˜å…ˆçº§è§£æå¯ç”¨åŸŸåï¼šç¼“å­˜æ–‡ä»¶ > ç¯å¢ƒå˜é‡ > æœ¬åœ°å˜é‡ > é»˜è®¤å€¼ > è‡ªåŠ¨å‘ç°"""
    print_with_time("å¼€å§‹åŸŸåè§£æ...", "INFO")

    # æ„å»ºå€™é€‰åˆ—è¡¨ï¼ˆæœ‰åºå»é‡ï¼‰
    candidates = []
    for domain in [read_domain_from_file(), os.getenv('IKUUU_DOMAIN'), LOCAL_DOMAIN, DEFAULT_DOMAIN]:
        if domain and domain not in candidates:
            candidates.append(domain)

    # é€ä¸ªæµ‹è¯•
    for domain in candidates:
        if test_domain(domain):
            _set_domain(domain)
            save_domain_to_file(domain)
            print_with_time(f"ä½¿ç”¨åŸŸå: {domain}", "SUCCESS")
            return domain

    # å€™é€‰åŸŸåå‡ä¸å¯ç”¨ï¼Œè‡ªåŠ¨å‘ç°
    print_with_time("å€™é€‰åŸŸåä¸å¯ç”¨ï¼Œå°è¯•è‡ªåŠ¨å‘ç°...", "WARNING")
    for domain in discover_domains():
        if domain not in candidates and test_domain(domain):
            _set_domain(domain)
            save_domain_to_file(domain)
            print_with_time(f"ä½¿ç”¨è‡ªåŠ¨å‘ç°çš„åŸŸå: {domain}", "SUCCESS")
            return domain

    print_with_time(f"æ‰€æœ‰åŸŸåå‡ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤åŸŸå: {DEFAULT_DOMAIN}", "ERROR")
    _set_domain(DEFAULT_DOMAIN)

def safe_request(method, url, **kwargs):
    """å®‰å…¨çš„ç½‘ç»œè¯·æ±‚ï¼ŒåŒ…å«é‡è¯•"""
    kwargs.setdefault('timeout', 8)
    kwargs['verify'] = False

    for attempt in range(2):
        try:
            if attempt > 0:
                time.sleep(attempt * 2)
                print_with_time(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯•...", "WARNING")
            session = create_session()
            response = session.request(method, url, **kwargs)
            session.close()
            return response
        except KeyboardInterrupt:
            raise
        except Exception as e:
            if attempt == 1:
                print_with_time(f"è¯·æ±‚å¤±è´¥: {str(e)}", "ERROR")
    return None

def login_and_get_cookie():
    """ç™»å½• SSPanel å¹¶è·å– Cookie"""
    email = os.getenv('IKUUU_EMAIL') or LOCAL_EMAIL
    password = os.getenv('IKUUU_PASSWORD') or LOCAL_PASSWORD

    if not email or not password:
        print_with_time("è¯·è®¾ç½®è´¦æˆ·ä¿¡æ¯ï¼ˆç¯å¢ƒå˜é‡ IKUUU_EMAIL/IKUUU_PASSWORD æˆ–ä»£ç ä¸­ LOCAL_EMAIL/LOCAL_PASSWORDï¼‰", "ERROR")
        return None

    masked_email = f"{email[:3]}***{email.split('@')[1]}"
    print_with_time(f"è´¦å·: {masked_email}ï¼ŒåŸŸå: {BASE_DOMAIN}", "INFO")

    session = create_session()
    try:
        # è·å–ç™»å½•é¡µé¢ï¼ˆå–CSRF tokenï¼‰
        login_page_url = f"{BASE_URL}/auth/login"
        try:
            response = session.get(login_page_url, timeout=8, verify=False)
        except Exception as e:
            print_with_time(f"è·å–ç™»å½•é¡µé¢å¤±è´¥: {str(e)}", "ERROR")
            return None

        if response.status_code != 200:
            print_with_time(f"æ— æ³•è®¿é—®ç™»å½•é¡µé¢ï¼ŒçŠ¶æ€ç : {response.status_code}", "ERROR")
            return None

        # æŸ¥æ‰¾ CSRF token
        soup = BeautifulSoup(response.text, 'html.parser')
        csrf_input = soup.find('input', {'name': '_token'})

        login_data = {'email': email, 'passwd': password}
        if csrf_input:
            login_data['_token'] = csrf_input.get('value')

        # å‘é€ç™»å½•è¯·æ±‚
        print_with_time("æ­£åœ¨ç™»å½•...", "INFO")
        headers = {
            'Origin': BASE_URL,
            'Referer': login_page_url,
            'Content-Type': 'application/x-www-form-urlencoded'
        }

        try:
            response = session.post(login_page_url, data=login_data, headers=headers,
                                    timeout=8, verify=False, allow_redirects=False)
        except Exception as e:
            print_with_time(f"ç™»å½•è¯·æ±‚å¤±è´¥: {str(e)}", "ERROR")
            return None

        cookie_string = '; '.join([f"{c.name}={c.value}" for c in session.cookies])

        # æ£€æŸ¥ç™»å½•ç»“æœ
        if response.status_code == 302 and '/user' in response.headers.get('Location', ''):
            print_with_time("ç™»å½•æˆåŠŸ", "SUCCESS")
            return cookie_string or None

        if response.status_code == 200:
            try:
                result = parse_json_response(response, "ç™»å½•")
                if result.get('ret') == 1:
                    print_with_time("ç™»å½•æˆåŠŸ", "SUCCESS")
                    return cookie_string or None
                else:
                    print_with_time(f"ç™»å½•å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}", "ERROR")
                    return None
            except Exception:
                # JSONè§£æå¤±è´¥ï¼Œç”¨Cookieåˆ¤æ–­
                if cookie_string:
                    print_with_time("ç™»å½•æˆåŠŸï¼ˆCookieæ£€æµ‹ï¼‰", "SUCCESS")
                    return cookie_string

        print_with_time(f"ç™»å½•å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}", "ERROR")
        return None
    except KeyboardInterrupt:
        raise
    except Exception as e:
        print_with_time(f"ç™»å½•é”™è¯¯: {str(e)}", "ERROR")
        return None
    finally:
        session.close()

def checkin(cookie):
    """æ‰§è¡Œç­¾åˆ°æ“ä½œ"""
    print_with_time("å¼€å§‹æ‰§è¡Œç­¾åˆ°...", "INFO")
    headers = {
        'Origin': BASE_URL, 'Referer': f"{BASE_URL}/user",
        'Cookie': cookie, 'X-Requested-With': 'XMLHttpRequest',
        'Content-Type': 'application/x-www-form-urlencoded'
    }

    try:
        response = safe_request('POST', f"{BASE_URL}/user/checkin", headers=headers)
        if not response:
            print_with_time("ç­¾åˆ°è¯·æ±‚å¤±è´¥", "ERROR")
            return False

        data = parse_json_response(response, "ç­¾åˆ°")
        msg = data.get('msg', '')
        if data.get('ret') == 1:
            print_with_time(f"ç­¾åˆ°æˆåŠŸ: {msg}", "SUCCESS")
            return True
        elif "å·²ç»ç­¾åˆ°" in msg:
            print_with_time(f"ä»Šæ—¥å·²ç­¾åˆ°: {msg}", "WARNING")
            return True
        else:
            print_with_time(f"ç­¾åˆ°å¤±è´¥: {msg or 'æœªçŸ¥é”™è¯¯'}", "ERROR")
            return False
    except KeyboardInterrupt:
        raise
    except Exception as e:
        print_with_time(f"ç­¾åˆ°å¤±è´¥: {str(e)}", "ERROR")
        return False

def extract_account_info(soup):
    """ä»è§£æçš„HTMLä¸­æå–è´¦æˆ·ä¿¡æ¯"""
    # å…³é”®è¯ -> æ˜¾ç¤ºæ ‡ç­¾æ˜ å°„
    label_map = [
        (['ä¼šå‘˜æ—¶é•¿', 'æ—¶é•¿', 'åˆ°æœŸ'], 'ä¼šå‘˜çŠ¶æ€'),
        (['å‰©ä½™æµé‡', 'æµé‡', 'å¯ç”¨'], 'å‰©ä½™æµé‡'),
        (['åœ¨çº¿è®¾å¤‡', 'è®¾å¤‡', 'è¿æ¥'], 'åœ¨çº¿è®¾å¤‡'),
        (['é’±åŒ…', 'ä½™é¢', 'ç§¯åˆ†'], 'è´¦æˆ·ä½™é¢'),
    ]

    stat_cards = (soup.find_all('div', class_='card-statistic-2')
                  or soup.find_all('div', class_='card-statistic')
                  or soup.find_all('div', class_='card'))

    info_found = False
    for card in stat_cards:
        header = card.find('h4') or card.find('h3') or card.find('h5')
        if not header:
            continue
        title = header.get_text(strip=True)
        body = card.find('div', class_='card-body') or card.find('div', class_='card-content')
        if not body:
            continue

        value = re.sub(r'\s+', ' ', body.get_text(strip=True))
        label = None
        for keywords, lbl in label_map:
            if any(k in title for k in keywords):
                label = lbl
                break

        if label:
            print(f"  {label}: {value}")
            info_found = True
        elif value and len(value) > 3:
            print(f"  {title.rstrip(':')}: {value}")
            info_found = True

    return info_found

def get_user_info(cookie):
    """è·å–ç”¨æˆ·ä¿¡æ¯å’Œæµé‡æ•°æ®"""
    print_separator("â”€", 50)
    print_with_time("æ­£åœ¨è·å–è´¦æˆ·ä¿¡æ¯...", "INFO")
    
    headers = {
        'Cookie': cookie
    }
    url = f"{BASE_URL}/user"
    
    try:
        response = safe_request('GET', url, headers=headers)
        
        if not response:
            print_with_time("è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥", "ERROR")
            return False
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ£€æŸ¥é¡µé¢æ ‡é¢˜ç¡®è®¤ç™»å½•çŠ¶æ€
        page_title = soup.find('title')
        if page_title:
            title_text = page_title.get_text(strip=True)
            if any(keyword in title_text.lower() for keyword in ['login', 'ç™»å½•']):
                print_with_time("ç™»å½•çŠ¶æ€å·²å¤±æ•ˆï¼Œè¯·æ£€æŸ¥è´¦æˆ·ä¿¡æ¯", "ERROR")
                return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Base64ç¼–ç çš„å†…å®¹
        decoded_html = None
        for script in soup.find_all('script'):
            script_content = script.get_text()
            if 'originBody' in script_content and 'decodeBase64' in script_content:
                match = re.search(r'var originBody = "([^"]+)"', script_content)
                if match:
                    try:
                        decoded_html = base64.b64decode(match.group(1)).decode('utf-8')
                    except Exception:
                        pass
                    break

        target_soup = BeautifulSoup(decoded_html, 'html.parser') if decoded_html else soup
        info_extracted = extract_account_info(target_soup)

        if not info_extracted:
            print_with_time("æœªèƒ½æå–åˆ°è¯¦ç»†è´¦æˆ·ä¿¡æ¯", "WARNING")
            all_text = decoded_html or soup.get_text()
            numbers = re.findall(r'(\d+(?:\.\d+)?)\s*(GB|MB|å¤©|ä¸ª|USD|CNY)', all_text)
            if numbers:
                for value, unit in set(numbers):
                    print(f"  {value} {unit}")
        
        print_separator("â”€", 50)
        return True
        
    except KeyboardInterrupt:
        print_with_time("ç”¨æˆ·ä¸­æ–­ä¿¡æ¯è·å–æ“ä½œ", "WARNING")
        raise
    except Exception as e:
        print_with_time(f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}", "ERROR")
        return False

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print_separator("=", 60)
    print_with_time("è‡ªåŠ¨ç­¾åˆ°ç¨‹åºå¯åŠ¨", "INFO")
    print_separator("=", 60)

    if not check_dependencies():
        return False

    resolve_domain()

    start_time = time.time()

    # ç™»å½•
    cookie_data = login_and_get_cookie()
    if not cookie_data:
        # ç™»å½•å¤±è´¥ï¼Œå°è¯•å…¶ä»–åŸŸå
        print_with_time("ç™»å½•å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢åŸŸå...", "WARNING")
        original = BASE_DOMAIN
        for domain in discover_domains():
            if domain != original and test_domain(domain):
                _set_domain(domain)
                save_domain_to_file(domain)
                print_with_time(f"åˆ‡æ¢åˆ° {domain}ï¼Œé‡è¯•ç™»å½•...", "INFO")
                cookie_data = login_and_get_cookie()
                if cookie_data:
                    break

    if not cookie_data:
        print_with_time("æ‰€æœ‰åŸŸåå‡æ— æ³•ç™»å½•", "ERROR")
        return False

    time.sleep(1)
    checkin_result = checkin(cookie_data)
    time.sleep(1)
    get_user_info(cookie_data)

    elapsed = round(time.time() - start_time, 2)
    print_separator("=", 60)
    print_with_time(f"æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {elapsed} ç§’", "SUCCESS" if checkin_result else "ERROR")
    print_separator("=", 60)
    return checkin_result

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)